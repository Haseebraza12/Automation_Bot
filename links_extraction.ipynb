{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exraction For PDF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links have been successfully saved to extracted.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Specify the file path directly\n",
    "pdf_file = \"1) Monthly Code Violations Open Records Request List-2.pdf\"\n",
    "\n",
    "def extract_links_from_pdf(pdf_file):\n",
    "    \"\"\"\n",
    "    Extract all hyperlinks from a PDF file.\n",
    "\n",
    "    :param pdf_file: Path to the PDF file.\n",
    "    :return: List of hyperlinks found in the PDF.\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    try:\n",
    "        reader = PdfReader(pdf_file)\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                # Regular expression to find URLs\n",
    "                urls = re.findall(r'(https?://[^\\s]+)', text)\n",
    "                links.extend(urls)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "    return links\n",
    "\n",
    "def save_links_to_csv(links, output_file):\n",
    "    \"\"\"\n",
    "    Save extracted links to a CSV file.\n",
    "\n",
    "    :param links: List of links.\n",
    "    :param output_file: Path to the output CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Links\"])  # Header\n",
    "            for link in links:\n",
    "                writer.writerow([link])\n",
    "        print(f\"Links have been successfully saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to CSV: {e}\")\n",
    "\n",
    "def main():\n",
    "    output_file = \"extracted.csv\"\n",
    "\n",
    "    links = extract_links_from_pdf(pdf_file)\n",
    "    if links:\n",
    "        save_links_to_csv(links, output_file)\n",
    "    else:\n",
    "        print(\"No links found in the PDF.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error accessing https://maconbibbcountyga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87d: 403 Client Error: Forbidden for url: https://maconbibbcountyga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87d\n",
      "Error accessing https://cantonga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://cantonga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://woodstockga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://woodstockga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://riverdalega.justfoia.com/publicportal/home/newrequest: 403 Client Error: Forbidden for url: https://riverdalega.justfoia.com/publicportal/home/newrequest\n",
      "Error accessing https://forestparkga.justfoia.com/publicportal/home/newrequest: 403 Client Error: Forbidden for url: https://forestparkga.justfoia.com/publicportal/home/newrequest\n",
      "Error accessing https://austellga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://austellga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://acworthga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://acworthga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://kennesawga.justfoia.com/Forms/Launch/a3570e65-d44d-43d3-a822-d38e2fc1c3d3: 403 Client Error: Forbidden for url: https://kennesawga.justfoia.com/Forms/Launch/a3570e65-d44d-43d3-a822-d38e2fc1c3d3\n",
      "Error accessing https://smyrnaga.justfoia.com/Forms/Launch/fd208f47-7557-4edf-9478-723c87ba6b30: 403 Client Error: Forbidden for url: https://smyrnaga.justfoia.com/Forms/Launch/fd208f47-7557-4edf-9478-723c87ba6b30\n",
      "Error accessing https://powderspringsga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://powderspringsga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://tuckerga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://tuckerga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://avondaleestates-open-records.app.transform.civicplus.com/forms/34577: 403 Client Error: Forbidden for url: https://avondaleestates-open-records.app.transform.civicplus.com/forms/34577\n",
      "Error accessing https://doravillega.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87de: 403 Client Error: Forbidden for url: https://doravillega.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87de\n",
      "Error accessing https://albanyga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://albanyga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://cityof: HTTPSConnectionPool(host='cityof', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000204DB448FB0>: Failed to resolve 'cityof' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error accessing https://tyronega.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://tyronega.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://forsythcountyga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87: 403 Client Error: Forbidden for url: https://forsythcountyga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87\n",
      "Error accessing https://sandyspringsga.justfoia.com/Forms/Launch/ab66f36f-7bcb-46ad-98c7-8e6a4dc7: 403 Client Error: Forbidden for url: https://sandyspringsga.justfoia.com/Forms/Launch/ab66f36f-7bcb-46ad-98c7-8e6a4dc7\n",
      "Error accessing https://eastpointga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87d: 403 Client Error: Forbidden for url: https://eastpointga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87d\n",
      "Error accessing https://fairburnga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87de: 403 Client Error: Forbidden for url: https://fairburnga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87de\n",
      "Error accessing https://hapevillega.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87d: 403 Client Error: Forbidden for url: https://hapevillega.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87d\n",
      "Error accessing https://collegeparkga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://collegeparkga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://stockbridgega.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://stockbridgega.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://www: HTTPSConnectionPool(host='www', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000204DB90F230>: Failed to resolve 'www' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error accessing https://cityofcovington.org/corecode/uploads/document6/uploaded_pdfs/corecode/PZ_Open%20Rec: 404 Client Error: Not Found for url: https://cityofcovington.org/corecode/uploads/document6/uploaded_pdfs/corecode/PZ_Open%20Rec\n",
      "Error accessing https://conyersga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://conyersga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://spaldingcountyga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://spaldingcountyga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://www.cityofgriffin.com/services/open-records: 403 Client Error: Forbidden for url: https://www.cityofgriffin.com/services/open-records\n",
      "Filtered links saved to links_with_captchas.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def check_security(url):\n",
    "    \"\"\"\n",
    "    Check if a URL has Cloudflare protection or reCAPTCHA.\n",
    "    :param url: URL to check.\n",
    "    :return: True if Cloudflare or reCAPTCHA is detected, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Check for Cloudflare in headers or page content\n",
    "        if \"cloudflare\" in response.headers.get(\"Server\", \"\").lower() or \"cloudflare\" in response.text.lower():\n",
    "            return True\n",
    "\n",
    "        # Check for reCAPTCHA keywords in page content\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        if soup.find(\"div\", {\"class\": \"g-recaptcha\"}) or \"recaptcha\" in response.text.lower():\n",
    "            return True\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "    return False\n",
    "\n",
    "def process_links(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Process the links from input CSV, check for security, and save results to output CSV.\n",
    "    :param input_csv: Path to the input CSV file containing URLs.\n",
    "    :param output_csv: Path to the output CSV file for links with security features.\n",
    "    \"\"\"\n",
    "    links_with_captchas = []\n",
    "\n",
    "    try:\n",
    "        # Read links from input CSV\n",
    "        with open(input_csv, mode='r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)  # Skip header\n",
    "            for row in reader:\n",
    "                if row:\n",
    "                    url = row[0]\n",
    "                    if check_security(url):\n",
    "                        links_with_captchas.append(url)\n",
    "\n",
    "        # Write filtered links to output CSV\n",
    "        with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Links with Captchas or Cloudflare\"])  # Header\n",
    "            for link in links_with_captchas:\n",
    "                writer.writerow([link])\n",
    "\n",
    "        print(f\"Filtered links saved to {output_csv}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing links: {e}\")\n",
    "\n",
    "def main():\n",
    "    input_csv = \"extracted.csv\"\n",
    "    output_csv = \"links_with_captchas.csv\"\n",
    "    process_links(input_csv, output_csv)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exraction For PDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links have been successfully saved to extracted_for_second_pdf.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Specify the file path directly\n",
    "pdf_file = \"Water Shut Off List-3 (1).pdf\"\n",
    "\n",
    "def extract_links_from_pdf(pdf_file):\n",
    "    \"\"\"\n",
    "    Extract all hyperlinks from a PDF file.\n",
    "\n",
    "    :param pdf_file: Path to the PDF file.\n",
    "    :return: List of hyperlinks found in the PDF.\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    try:\n",
    "        reader = PdfReader(pdf_file)\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                # Regular expression to find URLs\n",
    "                urls = re.findall(r'(https?://[^\\s]+)', text)\n",
    "                links.extend(urls)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "    return links\n",
    "\n",
    "def save_links_to_csv(links, output_file):\n",
    "    \"\"\"\n",
    "    Save extracted links to a CSV file.\n",
    "\n",
    "    :param links: List of links.\n",
    "    :param output_file: Path to the output CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Links\"])  # Header\n",
    "            for link in links:\n",
    "                writer.writerow([link])\n",
    "        print(f\"Links have been successfully saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to CSV: {e}\")\n",
    "\n",
    "def main():\n",
    "    output_file = \"extracted_for_second_pdf.csv\"\n",
    "\n",
    "    links = extract_links_from_pdf(pdf_file)\n",
    "    if links:\n",
    "        save_links_to_csv(links, output_file)\n",
    "    else:\n",
    "        print(\"No links found in the PDF.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error accessing https://maconbibbcountyga.justfoia.com/publicportal/home/newrequest: 403 Client Error: Forbidden for url: https://maconbibbcountyga.justfoia.com/publicportal/home/newrequest\n",
      "Error accessing https://cantonga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://cantonga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://woodstockga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://woodstockga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://cobbcountyga.govqa.us/WEBAPP/_rs/(S(md1ocphangyv2bbwxhwgpiuu))/RequestLogin.: 400 Client Error: Bad Request for url: https://cobbcountyga.govqa.us/WEBAPP/_rs/(S(md1ocphangyv2bbwxhwgpiuu))/RequestLogin.\n",
      "Error accessing https://smyrnaga.justfoia.com/Forms/Launch/fd208f47-7557-4edf-9478-723c87ba6b30: 403 Client Error: Forbidden for url: https://smyrnaga.justfoia.com/Forms/Launch/fd208f47-7557-4edf-9478-723c87ba6b30\n",
      "Error accessing https://austellga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://austellga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://albanyga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://albanyga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://forsythcountyga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://forsythcountyga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://www.atlantawatershed.org/openrecords/: 503 Server Error: Service Unavailable for url: https://www.atlantawatershed.org/openrecords/\n",
      "Error accessing https://roswellga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://roswellga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://fairburnga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://fairburnga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://collegeparkga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://collegeparkga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://henrycounty-services.app.transform.civicplus.com/forms/34175: 403 Client Error: Forbidden for url: https://henrycounty-services.app.transform.civicplus.com/forms/34175\n",
      "Error accessing https://www: HTTPSConnectionPool(host='www', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000204DB450050>: Failed to resolve 'www' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error accessing https://cityofcovington.org/corecode/uploads/document6/uploaded_pdfs/corecode/City_Open%20Re: 404 Client Error: Not Found for url: https://cityofcovington.org/corecode/uploads/document6/uploaded_pdfs/corecode/City_Open%20Re\n",
      "Error accessing https://conyersga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://conyersga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://spaldingcountyga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb: 403 Client Error: Forbidden for url: https://spaldingcountyga.justfoia.com/Forms/Launch/d705cbd6-1396-49b7-939e-8d86c5a87deb\n",
      "Error accessing https://www.cityofgriffin.com/services/open-records: 403 Client Error: Forbidden for url: https://www.cityofgriffin.com/services/open-records\n",
      "Filtered links saved to links_with_captchas_for_second_pdf.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def check_security(url):\n",
    "    \"\"\"\n",
    "    Check if a URL has Cloudflare protection or reCAPTCHA.\n",
    "    :param url: URL to check.\n",
    "    :return: True if Cloudflare or reCAPTCHA is detected, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Check for Cloudflare in headers or page content\n",
    "        if \"cloudflare\" in response.headers.get(\"Server\", \"\").lower() or \"cloudflare\" in response.text.lower():\n",
    "            return True\n",
    "\n",
    "        # Check for reCAPTCHA keywords in page content\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        if soup.find(\"div\", {\"class\": \"g-recaptcha\"}) or \"recaptcha\" in response.text.lower():\n",
    "            return True\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "    return False\n",
    "\n",
    "def process_links(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Process the links from input CSV, check for security, and save results to output CSV.\n",
    "    :param input_csv: Path to the input CSV file containing URLs.\n",
    "    :param output_csv: Path to the output CSV file for links with security features.\n",
    "    \"\"\"\n",
    "    links_with_captchas = []\n",
    "\n",
    "    try:\n",
    "        # Read links from input CSV\n",
    "        with open(input_csv, mode='r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)  # Skip header\n",
    "            for row in reader:\n",
    "                if row:\n",
    "                    url = row[0]\n",
    "                    if check_security(url):\n",
    "                        links_with_captchas.append(url)\n",
    "\n",
    "        # Write filtered links to output CSV\n",
    "        with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Links with Captchas or Cloudflare\"])  # Header\n",
    "            for link in links_with_captchas:\n",
    "                writer.writerow([link])\n",
    "\n",
    "        print(f\"Filtered links saved to {output_csv}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing links: {e}\")\n",
    "\n",
    "def main():\n",
    "    input_csv = \"extracted_for_second_pdf.csv\"\n",
    "    output_csv = \"links_with_captchas_for_second_pdf.csv\"\n",
    "    process_links(input_csv, output_csv)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PDF3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links have been successfully saved to extracted_for_third_pdf.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Specify the file path directly\n",
    "pdf_file = \"Fire Damage List  (2).pdf\"\n",
    "\n",
    "def extract_links_from_pdf(pdf_file):\n",
    "    \"\"\"\n",
    "    Extract all hyperlinks from a PDF file.\n",
    "\n",
    "    :param pdf_file: Path to the PDF file.\n",
    "    :return: List of hyperlinks found in the PDF.\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    try:\n",
    "        reader = PdfReader(pdf_file)\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                # Regular expression to find URLs\n",
    "                urls = re.findall(r'(https?://[^\\s]+)', text)\n",
    "                links.extend(urls)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "    return links\n",
    "\n",
    "def save_links_to_csv(links, output_file):\n",
    "    \"\"\"\n",
    "    Save extracted links to a CSV file.\n",
    "\n",
    "    :param links: List of links.\n",
    "    :param output_file: Path to the output CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Links\"])  # Header\n",
    "            for link in links:\n",
    "                writer.writerow([link])\n",
    "        print(f\"Links have been successfully saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to CSV: {e}\")\n",
    "\n",
    "def main():\n",
    "    output_file = \"extracted_for_third_pdf.csv\"\n",
    "\n",
    "    links = extract_links_from_pdf(pdf_file)\n",
    "    if links:\n",
    "        save_links_to_csv(links, output_file)\n",
    "    else:\n",
    "        print(\"No links found in the PDF.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error accessing https://maconbibbcountyga.justfoia.com/Forms/Launch/a709d888: 403 Client Error: Forbidden for url: https://maconbibbcountyga.justfoia.com/Forms/Launch/a709d888\n",
      "Error accessing https://cantonga.justfoia.com/Forms/Launch/d705cbd6: 403 Client Error: Forbidden for url: https://cantonga.justfoia.com/Forms/Launch/d705cbd6\n",
      "Error accessing https://woodstockga.justfoia.com/Forms/Launch/d705cbd6: 403 Client Error: Forbidden for url: https://woodstockga.justfoia.com/Forms/Launch/d705cbd6\n",
      "Error accessing https://riverdalega.justfoia.com/Forms/Launch/d705cbd6: 403 Client Error: Forbidden for url: https://riverdalega.justfoia.com/Forms/Launch/d705cbd6\n",
      "Error accessing https://forestparkga.justfoia.com/Forms/Launch/c7a60d81: 403 Client Error: Forbidden for url: https://forestparkga.justfoia.com/Forms/Launch/c7a60d81\n",
      "Error accessing https://austellga.justfoia.com/Forms/Launch/d705cbd6: 403 Client Error: Forbidden for url: https://austellga.justfoia.com/Forms/Launch/d705cbd6\n",
      "Error accessing https://acworthga.justfoia.com/Forms/Launch/d705cbd6: 403 Client Error: Forbidden for url: https://acworthga.justfoia.com/Forms/Launch/d705cbd6\n",
      "Error accessing https://kennesawga.justfoia.com/: 403 Client Error: Forbidden for url: https://kennesawga.justfoia.com/\n",
      "Error accessing https://www.decaturga.com/media/20231: 403 Client Error: Forbidden for url: https://www.decaturga.com/media/20231\n",
      "Error accessing https://fs6.formsite.: HTTPSConnectionPool(host='fs6.formsite.', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000204DBE84620>: Failed to resolve 'fs6.formsite' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error accessing https://albanyga.justfoia.com/publicportal/home/track: 403 Client Error: Forbidden for url: https://albanyga.justfoia.com/publicportal/home/track\n",
      "Error accessing https://fayettecountyga.gov/911_communications/open: 404 Client Error: Not Found for url: https://fayettecountyga.gov/911_communications/open\n",
      "Error accessing https://fo: HTTPSConnectionPool(host='fo', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000204DBE855B0>: Failed to resolve 'fo' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error accessing https://alpharettaga.justfoia.com/Forms/Launch/39f66254: 403 Client Error: Forbidden for url: https://alpharettaga.justfoia.com/Forms/Launch/39f66254\n",
      "Error accessing https://roswellga.justfoia.com/Forms/Launch/02b6cc8e: 403 Client Error: Forbidden for url: https://roswellga.justfoia.com/Forms/Launch/02b6cc8e\n",
      "Error accessing https://s: HTTPSConnectionPool(host='s', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000204DBE864E0>: Failed to resolve 's' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error accessing https://eastpointga.justfoia.com/Forms/Launch/d705cbd6: 403 Client Error: Forbidden for url: https://eastpointga.justfoia.com/Forms/Launch/d705cbd6\n",
      "Error accessing https://fairburnga.justfoia.com/Forms/Launch/d705cbd6: 403 Client Error: Forbidden for url: https://fairburnga.justfoia.com/Forms/Launch/d705cbd6\n",
      "Error accessing https://unioncityga.justfoia.com/publicportal/home/newrequest: 403 Client Error: Forbidden for url: https://unioncityga.justfoia.com/publicportal/home/newrequest\n",
      "Error accessing https://hapevillega.justfoia.com/Forms/Launch/0e8fe236: 403 Client Error: Forbidden for url: https://hapevillega.justfoia.com/Forms/Launch/0e8fe236\n",
      "Error accessing https://collegeparkga.justfoia.com/publicportal/home/newrequest: 403 Client Error: Forbidden for url: https://collegeparkga.justfoia.com/publicportal/home/newrequest\n",
      "Error accessing https://norcrossga.justfoia.com/Forms/Launch/2da76d30: 403 Client Error: Forbidden for url: https://norcrossga.justfoia.com/Forms/Launch/2da76d30\n",
      "Error accessing https://henrycounty: HTTPSConnectionPool(host='henrycounty', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000204DBE256D0>: Failed to resolve 'henrycounty' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error accessing https://www.columbusga.gov/fire/Open: 404 Client Error: Not Found for url: https://www.columbusga.gov/fire/Open\n",
      "Error accessing https://www.co.newton.ga.us/FormCenter/County: 404 Client Error: Not Found for url: https://www.newtoncountyga.gov/FormCenter/County\n",
      "Error accessing https://cityofcovington.org/corecode/uploads/document6/uploaded_pdfs/corecode/City_Open%20Re: 404 Client Error: Not Found for url: https://cityofcovington.org/corecode/uploads/document6/uploaded_pdfs/corecode/City_Open%20Re\n",
      "Error accessing https://c: HTTPSConnectionPool(host='c', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000204DBE27110>: Failed to resolve 'c' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error accessing https://spaldingcountyga.justfoia.com/Forms/Launch/d705cbd6: 403 Client Error: Forbidden for url: https://spaldingcountyga.justfoia.com/Forms/Launch/d705cbd6\n",
      "Error accessing https://www.cityofgriffin.com/services/open: 403 Client Error: Forbidden for url: https://www.cityofgriffin.com/services/open\n",
      "Filtered links saved to links_with_captchas_for_third_pdf.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def check_security(url):\n",
    "    \"\"\"\n",
    "    Check if a URL has Cloudflare protection or reCAPTCHA.\n",
    "    :param url: URL to check.\n",
    "    :return: True if Cloudflare or reCAPTCHA is detected, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Check for Cloudflare in headers or page content\n",
    "        if \"cloudflare\" in response.headers.get(\"Server\", \"\").lower() or \"cloudflare\" in response.text.lower():\n",
    "            return True\n",
    "\n",
    "        # Check for reCAPTCHA keywords in page content\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        if soup.find(\"div\", {\"class\": \"g-recaptcha\"}) or \"recaptcha\" in response.text.lower():\n",
    "            return True\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "    return False\n",
    "\n",
    "def process_links(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Process the links from input CSV, check for security, and save results to output CSV.\n",
    "    :param input_csv: Path to the input CSV file containing URLs.\n",
    "    :param output_csv: Path to the output CSV file for links with security features.\n",
    "    \"\"\"\n",
    "    links_with_captchas = []\n",
    "\n",
    "    try:\n",
    "        # Read links from input CSV\n",
    "        with open(input_csv, mode='r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)  # Skip header\n",
    "            for row in reader:\n",
    "                if row:\n",
    "                    url = row[0]\n",
    "                    if check_security(url):\n",
    "                        links_with_captchas.append(url)\n",
    "\n",
    "        # Write filtered links to output CSV\n",
    "        with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Links with Captchas or Cloudflare\"])  # Header\n",
    "            for link in links_with_captchas:\n",
    "                writer.writerow([link])\n",
    "\n",
    "        print(f\"Filtered links saved to {output_csv}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing links: {e}\")\n",
    "\n",
    "def main():\n",
    "    input_csv = \"extracted_for_third_pdf.csv\"\n",
    "    output_csv = \"links_with_captchas_for_third_pdf.csv\"\n",
    "    process_links(input_csv, output_csv)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Links in First Pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar links have been saved to similar_links_1.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import difflib\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def classify_links(input_csv, output_csv, similarity_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Classify similar links from an input CSV and save them to an output CSV.\n",
    "    \n",
    "    :param input_csv: Path to the input CSV file containing links.\n",
    "    :param output_csv: Path to the output CSV file for similar links.\n",
    "    :param similarity_threshold: Threshold for considering links as similar (0.0 to 1.0).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read links from the input CSV\n",
    "        with open(input_csv, mode='r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)  # Skip header\n",
    "            links = [row[0] for row in reader if row]\n",
    "\n",
    "        # Group similar links\n",
    "        grouped_links = []\n",
    "        visited = set()\n",
    "        \n",
    "        for i, link1 in enumerate(links):\n",
    "            if link1 in visited:\n",
    "                continue\n",
    "\n",
    "            group = [link1]\n",
    "            visited.add(link1)\n",
    "\n",
    "            for link2 in links[i+1:]:\n",
    "                if link2 not in visited:\n",
    "                    # Use difflib to calculate similarity\n",
    "                    similarity = difflib.SequenceMatcher(None, link1, link2).ratio()\n",
    "                    if similarity >= similarity_threshold:\n",
    "                        group.append(link2)\n",
    "                        visited.add(link2)\n",
    "\n",
    "            grouped_links.append(group)\n",
    "\n",
    "        # Write grouped links to the output CSV\n",
    "        with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Group\", \"Links\"])  # Header\n",
    "            for idx, group in enumerate(grouped_links, start=1):\n",
    "                writer.writerow([f\"Group {idx}\", \", \".join(group)])\n",
    "\n",
    "        print(f\"Similar links have been saved to {output_csv}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing links: {e}\")\n",
    "\n",
    "def main():\n",
    "    input_csv = \"extracted.csv\"\n",
    "    output_csv = \"similar_links_1.csv\"\n",
    "    classify_links(input_csv, output_csv)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar links have been saved to similar_links_2.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import difflib\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def classify_links(input_csv, output_csv, similarity_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Classify similar links from an input CSV and save them to an output CSV.\n",
    "    \n",
    "    :param input_csv: Path to the input CSV file containing links.\n",
    "    :param output_csv: Path to the output CSV file for similar links.\n",
    "    :param similarity_threshold: Threshold for considering links as similar (0.0 to 1.0).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read links from the input CSV\n",
    "        with open(input_csv, mode='r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)  # Skip header\n",
    "            links = [row[0] for row in reader if row]\n",
    "\n",
    "        # Group similar links\n",
    "        grouped_links = []\n",
    "        visited = set()\n",
    "        \n",
    "        for i, link1 in enumerate(links):\n",
    "            if link1 in visited:\n",
    "                continue\n",
    "\n",
    "            group = [link1]\n",
    "            visited.add(link1)\n",
    "\n",
    "            for link2 in links[i+1:]:\n",
    "                if link2 not in visited:\n",
    "                    # Use difflib to calculate similarity\n",
    "                    similarity = difflib.SequenceMatcher(None, link1, link2).ratio()\n",
    "                    if similarity >= similarity_threshold:\n",
    "                        group.append(link2)\n",
    "                        visited.add(link2)\n",
    "\n",
    "            grouped_links.append(group)\n",
    "\n",
    "        # Write grouped links to the output CSV\n",
    "        with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Group\", \"Links\"])  # Header\n",
    "            for idx, group in enumerate(grouped_links, start=1):\n",
    "                writer.writerow([f\"Group {idx}\", \", \".join(group)])\n",
    "\n",
    "        print(f\"Similar links have been saved to {output_csv}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing links: {e}\")\n",
    "\n",
    "def main():\n",
    "    input_csv = \"extracted_for_second_pdf.csv\"\n",
    "    output_csv = \"similar_links_2.csv\"\n",
    "    classify_links(input_csv, output_csv)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar links have been saved to similar_links_3.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import difflib\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def classify_links(input_csv, output_csv, similarity_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Classify similar links from an input CSV and save them to an output CSV.\n",
    "    \n",
    "    :param input_csv: Path to the input CSV file containing links.\n",
    "    :param output_csv: Path to the output CSV file for similar links.\n",
    "    :param similarity_threshold: Threshold for considering links as similar (0.0 to 1.0).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read links from the input CSV\n",
    "        with open(input_csv, mode='r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)  # Skip header\n",
    "            links = [row[0] for row in reader if row]\n",
    "\n",
    "        # Group similar links\n",
    "        grouped_links = []\n",
    "        visited = set()\n",
    "        \n",
    "        for i, link1 in enumerate(links):\n",
    "            if link1 in visited:\n",
    "                continue\n",
    "\n",
    "            group = [link1]\n",
    "            visited.add(link1)\n",
    "\n",
    "            for link2 in links[i+1:]:\n",
    "                if link2 not in visited:\n",
    "                    # Use difflib to calculate similarity\n",
    "                    similarity = difflib.SequenceMatcher(None, link1, link2).ratio()\n",
    "                    if similarity >= similarity_threshold:\n",
    "                        group.append(link2)\n",
    "                        visited.add(link2)\n",
    "\n",
    "            grouped_links.append(group)\n",
    "\n",
    "        # Write grouped links to the output CSV\n",
    "        with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Group\", \"Links\"])  # Header\n",
    "            for idx, group in enumerate(grouped_links, start=1):\n",
    "                writer.writerow([f\"Group {idx}\", \", \".join(group)])\n",
    "\n",
    "        print(f\"Similar links have been saved to {output_csv}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing links: {e}\")\n",
    "\n",
    "def main():\n",
    "    input_csv = \"extracted_for_third_pdf.csv\"\n",
    "    output_csv = \"similar_links_3.csv\"\n",
    "    classify_links(input_csv, output_csv)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
